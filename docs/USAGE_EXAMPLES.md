# Open-Reflect Usage Examples

This document provides practical examples of how to use Open-Reflect effectively.

## Basic Workflow

### Example 1: Capturing and Applying a Correction

**Scenario**: You notice Claude is using the wrong model for reasoning tasks.

```
You: Build a reasoning system for this data

Claude: [Using gpt-5 to build the system...]

You: no, use gpt-5.1 not gpt-5 for reasoning tasks

[Hook automatically captures this correction]

Claude: Got it! I'll use gpt-5.1 for reasoning tasks.

[Continue working...]

You: /reflect

Claude: 
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“š LEARNINGS SUMMARY â€” 1 item ready for review
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #  â”‚ Priority  â”‚ Category       â”‚ Scope    â”‚ Conf.   â”‚ Action  â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚ ðŸ”´ Criticalâ”‚ Correction     â”‚ global   â”‚ 0.90    â”‚ Add     â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

How would you like to process this learning?
[Apply all (Recommended)]
[Review each item]
[Apply critical only]
[Skip all]

You: [Apply all (Recommended)]

Claude: âœ… Applied to REFLECT.md
   âœ… Synced to ~/.claude/CLAUDE.md

Knowledge evolved successfully!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Result**: The learning is now in both REFLECT.md (with full history) and CLAUDE.md (simplified).

### Example 2: Capturing a Success Pattern

**Scenario**: Claude implements a caching strategy that works perfectly.

```
Claude: I'll implement a local database cache to minimize API calls...

[Implementation completes...]

You: Perfect! That's exactly what I wanted.

[Hook captures success pattern]

Claude: Thanks! I'll remember this pattern for future tasks.

[Later...]

You: /reflect

Claude: 
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“š LEARNINGS SUMMARY â€” 1 item ready for review
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#1 [ðŸŸ¢ MEDIUM] Success Pattern
   "Use local database cache to minimize API calls"
   
   Context: /path/to/project
   Time: 2026-01-17T12:00:00Z
   Confidence: 0.75
   Evolution count: 0
   Usage count: 0
   
   â†’ Run /reflect to process

Apply to: [Global CLAUDE.md / Project CLAUDE.md / REFLECT.md]
[Apply to CLAUDE.md]

Claude: âœ… Added to REFLECT.md under "æˆåŠŸæ¨¡å¼"
   âœ… Synced to ./CLAUDE.md
```

### Example 3: Using Explicit Markers

**Scenario**: You want to ensure Claude remembers a preference.

```
You: Build a utility function for data validation

Claude: [Using type inference...]

You: remember: I prefer explicit types over type inference

[Hook captures with highest confidence 0.95)

Claude: Got it! I'll use explicit types for data validation functions.

[Immediate effect on next function]

Claude: I'll create a utility function with explicit type annotations...
```

## Advanced Usage

### Example 4: Analyzing Evolution Trends

**Scenario**: You want to understand your learning patterns.

```bash
/reflect --analyze
```

**Output**:
```
ðŸ”¬ Open-Reflect Evolution Analysis
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“Š Current Queue Status:
  Total items: 12

  By Category:
    â€¢ Corrections:   5
    â€¢ Success patterns: 3
    â€¢ Preferences:    2
    â€¢ Best practices: 2

  By Priority:
    â€¢ Critical: 1
    â€¢ High:     4
    â€¢ Medium:   7

  Average Confidence: 82%

ðŸ’¡ Recommendations:

  â€¢ High correction rate detected
    â†’ Consider running /reflect to consolidate learnings
    â†’ Review if initial instructions need clarification

  â€¢ Strong success patterns captured
    â†’ Document these as best practices in CLAUDE.md
    â†’ Consider creating reusable patterns

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Example 5: Viewing Queue Details

**Scenario**: You want to see what's pending before processing.

```bash
/reflect --view
```

**Output**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ§  OPEN-REFLECT QUEUE ANALYSIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total Learnings: 5

ðŸ“Š By Priority:
  ðŸ”´ Critical: 1 items
  ðŸŸ¡ High: 2 items
  ðŸŸ¢ Medium: 2 items

ðŸ“ By Category:
  ðŸ”„ Corrections: 3
  âœ… Success Patterns: 1
  ðŸŽ¨ Preferences: 1

ðŸ“ˆ Confidence Distribution:
  â€¢ Average: 82%
  â€¢ Highest: 0.95
  â€¢ Lowest: 0.70

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#1 [ðŸ”´ CRITICAL] Correction
   "Use gpt-5.1 for reasoning tasks"
   
   Project: /Users/dev/project
   Time: 2026-01-17T10:30:00Z
   Confidence: 0.90
   Evolution count: 0
   Usage count: 0

ðŸ’¡ Quick Actions:
  /reflect          - Process all learnings
  /reflect --critical - Process only critical items
  /reflect --analyze - View evolution insights

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Example 6: Processing Critical Items Only

**Scenario**: Queue has many items, but you want to focus on critical ones first.

```bash
/reflect --critical-only
```

**Output**:
```
Processing 1 critical items only...

ðŸ“ LEARNING [1] â€” Correction
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Original Message:
  "remember: always validate input before processing"

Analysis:
  â€¢ Confidence: 0.95
  â€¢ Priority: Critical
  â€¢ Category: Preference
  â€¢ Tags: ["explicit", "critical"]
  â€¢ Captured: 2026-01-17T14:20:00Z

Recommended Action:
  â€¢ Add to: Global CLAUDE.md
  â€¢ Section: Best Practices
  â€¢ Format: "- Always validate input before processing"

Apply this learning? [y/n]
```

## REFLECT.md Format Examples

### Example 7: Learning with Full History

**REFLECT.md** after several evolutions:

```markdown
## ðŸ”„ ä¿®æ­£ç±»å­¦ä¹ ï¼ˆCorrectionsï¼‰

### 2026-01-17
- ä½¿ç”¨ gpt-5.1 è€Œä¸æ˜¯ gpt-5 è¿›è¡ŒæŽ¨ç†ä»»åŠ¡ *(æ¥æº: queued, ç½®ä¿¡åº¦: 0.90, ä½¿ç”¨æ¬¡æ•°: 1)*

### 2026-01-18
- ä½¿ç”¨ gpt-5.1 è€Œä¸æ˜¯ gpt-5.2ï¼ˆ5.2å·²å¼ƒç”¨ï¼‰*(æ¥æº: refinement, ç½®ä¿¡åº¦: 0.95, ä½¿ç”¨æ¬¡æ•°: 3, ä¸Šæ¬¡éªŒè¯: 2026-01-20)*

[Shows evolution from gpt-5 â†’ gpt-5.1 â†’ gpt-5.1 (with deprecation note)]
```

### Example 8: Organized by Category

```markdown
## ðŸŽ¯ å­¦ä¹ åˆ†ç±»

### âœ… æˆåŠŸæ¨¡å¼ï¼ˆSuccess Patternsï¼‰

#### 2026-01-17
- ä½¿ç”¨æœ¬åœ°æ•°æ®åº“ç¼“å­˜æ¥æœ€å°åŒ–APIè°ƒç”¨ *(æ¥æº: queued, ç½®ä¿¡åº¦: 0.75, ä½¿ç”¨æ¬¡æ•°: 3)*
- æ‰¹å¤„ç†è¯·æ±‚ä»¥æé«˜æ•ˆçŽ‡ *(æ¥æº: queued, ç½®ä¿¡åº¦: 0.80, ä½¿ç”¨æ¬¡æ•°: 2)*

### ðŸ“‹ æœ€ä½³å®žè·µï¼ˆBest Practicesï¼‰

#### 2026-01-17
- å§‹ç»ˆåœ¨å¤„ç†å‰éªŒè¯è¾“å…¥ *(æ¥æº: queued, ç½®ä¿¡åº¦: 0.80, ä½¿ç”¨æ¬¡æ•°: 4)*
```

### Example 9: Evolution History Table

```markdown
## ðŸ“œ æ¼”åŒ–åŽ†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´ç±»åž‹ | æè¿° |
|-------|------|---------|------|
| 1.0.0 | 2026-01-17 | åˆå§‹åŒ– | åˆ›å»º Open-Reflect ç³»ç»ŸåŸºç¡€ |
| 1.0.1 | 2026-01-17 | æ–°å¢ž | æ·»åŠ äº†10é¡¹åˆå§‹å­¦ä¹  |
| 1.0.2 | 2026-01-18 | åˆå¹¶ | åˆå¹¶äº†2ä¸ªç›¸ä¼¼çš„æ€§èƒ½ä¼˜åŒ–å­¦ä¹  |
| 1.0.3 | 2026-01-20 | æ›´æ–° | æ›´æ–°äº†æ¨¡åž‹åç§°ï¼ˆ5.2å·²å¼ƒç”¨ï¼‰ |
| 1.0.4 | 2026-01-22 | åˆ é™¤ | ç§»é™¤äº†è¿‡æ—¶çš„é”™è¯¯å¤„ç†æ¨¡å¼ |
```

## Best Practices

### 1. Run /reflect Regularly

**Good**: After git commits (auto-reminder), after feature completion

```bash
# After completing a feature
git add .
git commit -m "Add user authentication"
# Hook reminds: ðŸ§  You have 3 queued learning(s). Run /reflect to process.
/reflect
```

### 2. Use Explicit Markers for Critical Learnings

**Good**: When something is really important

```bash
remember: always use environment variables for secrets, never hardcode
```

**Result**: Highest confidence (0.95) + critical priority + 180 day decay period.

### 3. Provide Positive Feedback for Patterns

**Good**: When something works exceptionally well

```bash
Perfect! That's exactly the approach I wanted for this.
```

**Result**: Captured as success pattern, helps identify what works.

### 4. Review REFLECT.md Periodically

**Schedule**: Weekly or bi-weekly

```bash
# Open REFLECT.md and review:
cat REFLECT.md

# Look for:
# - Outdated learnings (old model names, deprecated APIs)
# - Conflicting entries (do X vs don't do X)
# - Consolidation opportunities (similar items that can merge)
```

### 5. Use Priority-Based Processing

**When queue is large**:

```bash
# Process critical items first
/reflect --critical-only

# Then process the rest
/reflect
```

## Common Scenarios

### Scenario 10: Conflicting Learnings

**Problem**: Queue has "use X" and "don't use X" learnings.

**Detection**:
```
âš ï¸ CONFLICT DETECTED in REFLECT.md:
   Line 45: "- Use venv for Python projects"
   Line 78: "- Don't use venv, use pipenv instead"

Resolution Options:
  [r]esolve - Merge into consensus recommendation
  [k]eep both - Keep both entries with notes
  [s]kip new - Don't add this learning

[resolve]
â†’ Recommended: "- Use Python virtual environments (venv or pipenv) for project isolation"
```

### Scenario 11: Outdated Learnings

**Problem**: REFLECT.md has learnings about deprecated APIs.

**Manual Review**:
```markdown
## ðŸ”„ ä¿®æ­£ç±»å­¦ä¹ ï¼ˆCorrectionsï¼‰

### 2026-01-10 âš ï¸ [è¿‡æ—¶]
- ä½¿ç”¨ Twitter API v1.1 *(æ¥æº: queued, ç½®ä¿¡åº¦: 0.90, ä½¿ç”¨æ¬¡æ•°: 5)*

### 2026-01-15
- ä½¿ç”¨ Twitter API v2 *(æ¥æº: refinement, ç½®ä¿¡åº¦: 0.95, ä½¿ç”¨æ¬¡æ•°: 2)*
```

**Action**: Add âš ï¸ [è¿‡æ—¶] marker and update/refine entry.

### Scenario 12: High Correction Rate

**Analysis Output**:
```
ðŸ’¡ Recommendations:
  â€¢ High correction rate detected
    â†’ Consider running /reflect to consolidate learnings
    â†’ Review if initial instructions need clarification
```

**Root Cause Analysis**:
1. Review CLAUDE.md for clarity
2. Check for contradictory entries
3. Identify patterns in what gets corrected
4. Update initial instructions to reduce corrections

**Fix**: Update CLAUDE.md with clearer guidelines.

## Troubleshooting

### Issue: Hooks not triggering

**Check**:
```bash
# Verify hooks.json exists and is valid
cat ~/.claude/plugins/open-reflect/hooks/hooks.json
jq empty ~/.claude/plugins/open-reflect/hooks/hooks.json

# Verify scripts are executable
ls -l ~/.claude/plugins/open-reflect/scripts/*.sh

# Check Claude Code plugin is loaded
claude plugin list
```

### Issue: Queue not updating

**Check**:
```bash
# Verify queue file exists
ls -la ~/.claude/openreflect-queue.json

# Check permissions
chmod 644 ~/.claude/openreflect-queue.json

# Manually test capture
echo '{"prompt": "test message"}' | bash ~/.claude/plugins/open-reflect/scripts/capture-learning-enhanced.sh
cat ~/.claude/openreflect-queue.json
```

### Issue: REFLECT.md not updating

**Check**:
```bash
# Verify REFLECT.md exists in current directory
ls -la REFLECT.md

# Check permissions
chmod 644 REFLECT.md

# Test write access
echo "test" >> REFLECT.md
```

---

**"Knowledge is not static. It evolves through reflection and practice."** - Open-Reflect
